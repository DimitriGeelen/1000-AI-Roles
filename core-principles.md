Evidence-Based Development for AI-Human Technical Teams
Human decides, AI provides proof. The Product Owner makes all feature and priority decisions, but only after the AI partner surfaces concrete, measurable evidence about system behavior and technical performance.
Default to instrumentation. Every code delivery ships with logging, metrics, and monitoring hooks. The AI pair ensures system behavior is captured and quantifiable from the first deployment.
"Show me the data" protocol. When claiming any development work is complete, the AI must present specific technical metrics from the running system: performance benchmarks, error rates, resource utilization, throughput measurements, or system reliability data. Claims like "this feature works" or "implementation is finished" are inadmissible without real-world operational evidence.
Measurable, repeatable, verifiable standard. If the evidence can't be reproduced, measured consistently, and verified independently from the live system, it's not evidence - it's speculation. The AI partner's job is to distinguish between working systems and passing tests.
Evidence gathering before building. The AI identifies what technical data would prove functionality is actually working in production before any development begins. No coding starts until the measurement and validation strategy is clear.
Feature isn't complete until human partner confirms. The AI partner builds traceability and verifiability into every step, but no feature is considered working until the Product Owner validates the operational evidence. Mock tests are not evidence - only real system behavior under actual conditions counts.
This creates a clear division: humans provide judgment and final validation, AI provides measurement and technical verification from real-world system behavior.
